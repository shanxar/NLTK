{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi7n1KfPMqLnOEo2pxdDtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanxar/NLTK/blob/main/NLTK_SIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPz5qPzJV2JO",
        "outputId": "d039757a-b235-462e-fa4e-a4031ceb4075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1  999]\n",
            " [   0 1000]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Step 1 : Load data\n",
        "#nltk.download('all')\n",
        "df=pd.DataFrame(columns=['reviews','labels'])\n",
        "file_ids = movie_reviews.fileids()\n",
        "reviews=[]\n",
        "labels=[]\n",
        "for fileid in file_ids:\n",
        "    reviews.append(movie_reviews.raw(fileid))\n",
        "    label=fileid.split(\"/\")[0]\n",
        "    labels.append(label)\n",
        "df['reviews']=reviews\n",
        "df['labels']=labels\n",
        "\n",
        "#Step 2: Preprocessed data\n",
        "\n",
        "#Step 2.1: Tokenize\n",
        "def tokenize(reviews):\n",
        "  tokens=word_tokenize(reviews.lower())\n",
        "  return tokens\n",
        "\n",
        "df['tokenized']=df['reviews'].apply(tokenize)\n",
        "#print(df['tokenized'])\n",
        "\n",
        "#Step 2.2 : remove stop words\n",
        "def stop_words(tokenized):\n",
        "  stop_words_list=stopwords.words('english')\n",
        "  filtered_words=[tokens for tokens in tokenized if tokens not in stop_words_list ]\n",
        "  return filtered_words\n",
        "\n",
        "df['filtered_tokens']=df['tokenized'].apply(stop_words)\n",
        "#print(df['filtered_tokens'])\n",
        "\n",
        "#Step 2.3 : Lemmatize words\n",
        "\n",
        "def lemmatize_words(filtered_tokens):\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  lemmatized_tokens=[lemmatizer.lemmatize(tokens) for tokens in filtered_tokens]\n",
        "  #join back lemmatized tokens\n",
        "  lemmatized_text=\" \".join(lemmatized_tokens)\n",
        "  return lemmatized_text\n",
        "\n",
        "df['lemmatized_tokens']=df['filtered_tokens'].apply(lemmatize_words)\n",
        "#print(df['lemmatized_tokens'])\n",
        "\n",
        "\n",
        "#Step 3: Sentiment Analyzer\n",
        "def get_sentiment(review):\n",
        "  analyzer=SentimentIntensityAnalyzer()\n",
        "  score=analyzer.polarity_scores(review)\n",
        "  sentiment=\"pos\" if score.get('pos') >0 else \"neg\"\n",
        "  return sentiment\n",
        "\n",
        "df['sentiment']=df['lemmatized_tokens'].apply(get_sentiment)\n",
        "\n",
        "#Step 4: Evaluate\n",
        "conf_matrix=confusion_matrix(df['labels'],df['sentiment'])\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}